{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\llamaindex\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import (\n",
    "    Settings,\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    ")\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from llama_index.core import Settings, VectorStoreIndex\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "import shutil\n",
    "from llama_parse import LlamaParse\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "import os\n",
    "from typing import List, Optional\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, EmailStr, Field, root_validator, field_validator\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import openai\n",
    "import numpy as np\n",
    "import json\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    Settings,\n",
    "    Document,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.core.node_parser import MarkdownElementNodeParser\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import streamlit as st\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio; nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_CLOUD_API_KEY = st.secrets[\"LLAMA_CLOUD_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16344\\2887394112.py:74: PydanticDeprecatedSince20: Pydantic V1 style `@root_validator` validators are deprecated. You should migrate to Pydantic V2 style `@model_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.6/migration/\n",
      "  @root_validator(pre=True)\n"
     ]
    }
   ],
   "source": [
    "# Pydantic model for extracting education\n",
    "class Education(BaseModel):\n",
    "    institution: Optional[str] = Field(\n",
    "        None, description=\"The name of the educational institution\"\n",
    "    )\n",
    "    degree: Optional[str] = Field(\n",
    "        None, description=\"The degree or qualification earned\"\n",
    "    )\n",
    "    graduation_date: Optional[str] = Field(\n",
    "        None, description=\"The graduation date (e.g., 'YYYY-MM')\"\n",
    "    )\n",
    "    details: Optional[List[str]] = Field(\n",
    "        None,\n",
    "        description=\"Additional details about the education (e.g., coursework, achievements)\",\n",
    "    )\n",
    "\n",
    "    @field_validator(\"details\", mode=\"before\")\n",
    "    def validate_details(cls, v):\n",
    "        if isinstance(v, str) and v.lower() == \"n/a\":\n",
    "            return []\n",
    "        elif not isinstance(v, list):\n",
    "            return []\n",
    "        return v\n",
    "\n",
    "\n",
    "# Pydantic model for extracting experience\n",
    "class Experience(BaseModel):\n",
    "    company: Optional[str] = Field(\n",
    "        None, description=\"The name of the company or organization\"\n",
    "    )\n",
    "    location: Optional[str] = Field(\n",
    "        None, description=\"The location of the company or organization\"\n",
    "    )\n",
    "    role: Optional[str] = Field(\n",
    "        None, description=\"The role or job title held by the candidate\"\n",
    "    )\n",
    "    start_date: Optional[str] = Field(\n",
    "        None, description=\"The start date of the job (e.g., 'YYYY-MM')\"\n",
    "    )\n",
    "    end_date: Optional[str] = Field(\n",
    "        None,\n",
    "        description=\"The end date of the job or 'Present' if ongoing (e.g., 'MM-YYYY')\",\n",
    "    )\n",
    "    responsibilities: Optional[List[str]] = Field(\n",
    "        None, description=\"A list of responsibilities and tasks handled during the job\"\n",
    "    )\n",
    "\n",
    "    @field_validator(\"responsibilities\", mode=\"before\")\n",
    "    def validate_responsibilities(cls, v):\n",
    "        if isinstance(v, str) and v.lower() == \"n/a\":\n",
    "            return []\n",
    "        elif not isinstance(v, list):\n",
    "            return []\n",
    "        return v\n",
    "\n",
    "\n",
    "# Main Pydantic class ensapsulating education and epxerience classes with other information\n",
    "class Candidate(BaseModel):\n",
    "    name: Optional[str] = Field(None, description=\"The full name of the candidate\")\n",
    "    email: Optional[EmailStr] = Field(None, description=\"The email of the candidate\")\n",
    "    age: Optional[int] = Field(None, description=\"The age of the candidate.\")\n",
    "    skills: Optional[List[str]] = Field(\n",
    "        None, description=\"A list of high-level skills possessed by the candidate.\"\n",
    "    )\n",
    "    experience: Optional[List[Experience]] = Field(\n",
    "        None,\n",
    "        description=\"A list of experiences detailing previous jobs, roles, and responsibilities\",\n",
    "    )\n",
    "    education: Optional[List[Education]] = Field(\n",
    "        None,\n",
    "        description=\"A list of educational qualifications of the candidate including degrees, institutions studied in, and dates of start and end.\",\n",
    "    )\n",
    "\n",
    "    @root_validator(pre=True)\n",
    "    def handle_invalid_values(cls, values):\n",
    "        for key, value in values.items():\n",
    "            if isinstance(value, str) and value.lower() in {\"n/a\", \"none\", \"\"}:\n",
    "                values[key] = None\n",
    "        return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "\n",
    "# Load environment variables and set OpenAI API key\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# llm = Ollama(model=\"llama3.2:1b\", request_timeout=300.0)\n",
    "llm = Ollama(model=\"llama3.2\", request_timeout=300.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "response = llm.complete(\"What is the capital of France?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting CV data. LLM: callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000002450DECF310> system_prompt=None messages_to_prompt=<function messages_to_prompt at 0x0000024500E4F0A0> completion_to_prompt=<function default_completion_to_prompt at 0x0000024500EDAB00> output_parser=None pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'> query_wrapper_prompt=None base_url='http://localhost:11434' model='llama3.2' temperature=0.75 context_window=3900 request_timeout=300.0 prompt_key='prompt' json_mode=False additional_kwargs={}\n",
      "Error while parsing the PDF file:  Failed to parse the PDF file: {\"detail\":[{\"type\":\"enum\",\"loc\":[\"body\",\"language\",0],\"msg\":\"Input should be 'af', 'az', 'bs', 'cs', 'cy', 'da', 'de', 'en', 'es', 'et', 'fr', 'ga', 'hr', 'hu', 'id', 'is', 'it', 'ku', 'la', 'lt', 'lv', 'mi', 'ms', 'mt', 'nl', 'no', 'oc', 'pi', 'pl', 'pt', 'ro', 'rs_latin', 'sk', 'sl', 'sq', 'sv', 'sw', 'tl', 'tr', 'uz', 'vi', 'ar', 'fa', 'ug', 'ur', 'bn', 'as', 'mni', 'ru', 'rs_cyrillic', 'be', 'bg', 'uk', 'mn', 'abq', 'ady', 'kbd', 'ava', 'dar', 'inh', 'che', 'lbe', 'lez', 'tab', 'tjk', 'hi', 'mr', 'ne', 'bh', 'mai', 'ang', 'bho', 'mah', 'sck', 'new', 'gom', 'sa', 'bgc', 'th', 'ch_sim', 'ch_tra', 'ja', 'ko', 'ta', 'te' or 'kn'\",\"input\":\"Language.ENGLISH\",\"ctx\":{\"expected\":\"'af', 'az', 'bs', 'cs', 'cy', 'da', 'de', 'en', 'es', 'et', 'fr', 'ga', 'hr', 'hu', 'id', 'is', 'it', 'ku', 'la', 'lt', 'lv', 'mi', 'ms', 'mt', 'nl', 'no', 'oc', 'pi', 'pl', 'pt', 'ro', 'rs_latin', 'sk', 'sl', 'sq', 'sv', 'sw', 'tl', 'tr', 'uz', 'vi', 'ar', 'fa', 'ug', 'ur', 'bn', 'as', 'mni', 'ru', 'rs_cyrillic', 'be', 'bg', 'uk', 'mn', 'abq', 'ady', 'kbd', 'ava', 'dar', 'inh', 'che', 'lbe', 'lez', 'tab', 'tjk', 'hi', 'mr', 'ne', 'bh', 'mai', 'ang', 'bho', 'mah', 'sck', 'new', 'gom', 'sa', 'bgc', 'th', 'ch_sim', 'ch_tra', 'ja', 'ko', 'ta', 'te' or 'kn'\"}}]}\n"
     ]
    }
   ],
   "source": [
    "llm_option = llm\n",
    "file_path = \"./Dang_Quang_Dung_CV__Copy___Copy_.pdf\"\n",
    "\n",
    "\"\"\"\n",
    "Extracts candidate data from the resume.\n",
    "\"\"\"\n",
    "print(f\"Extracting CV data. LLM: {llm_option}\")\n",
    "output_schema = Candidate.model_json_schema()\n",
    "parser = LlamaParse(\n",
    "    result_type=\"markdown\",\n",
    "    parsing_instructions=\"Extract each section separately based on the document structure.\",\n",
    "    premium_mode=True,\n",
    "    # api_key=os.getenv(\"LLAMA_CLOUD_API_KEY\"),\n",
    "    api_key=\"llx-OxbevJhMJFNaEWwoc5MstJ88kOn3s1ane6hwGzRzbSogArUS\",\n",
    "    verbose=True,\n",
    ")\n",
    "file_extractor = {\".pdf\": parser}\n",
    "\n",
    "# Load resume\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[file_path], file_extractor=file_extractor\n",
    ").load_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting CV data. LLM: callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000002450DECF310> system_prompt=None messages_to_prompt=<function messages_to_prompt at 0x0000024500E4F0A0> completion_to_prompt=<function default_completion_to_prompt at 0x0000024500EDAB00> output_parser=None pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'> query_wrapper_prompt=None base_url='http://localhost:11434' model='llama3.2' temperature=0.75 context_window=3900 request_timeout=300.0 prompt_key='prompt' json_mode=False additional_kwargs={}\n"
     ]
    }
   ],
   "source": [
    "llm_option = llm\n",
    "file_path = \"./Dang_Quang_Dung_CV__Copy___Copy_.pdf\"\n",
    "\n",
    "\"\"\"\n",
    "Extracts candidate data from the resume.\n",
    "\"\"\"\n",
    "print(f\"Extracting CV data. LLM: {llm_option}\")\n",
    "output_schema = Candidate.model_json_schema()\n",
    "# parser = LlamaParse(\n",
    "#     result_type=\"markdown\",\n",
    "#     parsing_instructions=\"Extract each section separately based on the document structure.\",\n",
    "#     premium_mode=True,\n",
    "#     # api_key=os.getenv(\"LLAMA_CLOUD_API_KEY\"),\n",
    "#     api_key=\"llx-OxbevJhMJFNaEWwoc5MstJ88kOn3s1ane6hwGzRzbSogArUS\",\n",
    "#     verbose=True,\n",
    "# )\n",
    "# file_extractor = {\".pdf\": parser}\n",
    "\n",
    "# Load resume\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[file_path]\n",
    ").load_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='0fb220c4-cdbf-40ba-999a-95556bf8da70', embedding=None, metadata={'page_label': '1', 'file_name': 'e:/Master-code/Job-recommend/CV-Analyzer-Job-Recommender/Multiple models/Dang_Quang_Dung_CV__Copy___Copy_.pdf', 'file_path': 'Dang_Quang_Dung_CV__Copy___Copy_.pdf', 'file_type': 'application/pdf', 'file_size': 763671, 'creation_date': '2025-01-19', 'last_modified_date': '2024-12-30', 'last_accessed_date': None}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Dang Quang Dung\\nanhdungvk01@gmail.com\\n0869040236\\nLinkedin\\nGithub\\nCau Giay - Ha Noi\\nProfile\\nInﬁveyears’time,IwanttobecomeanAIEngineer.MystartingpointistobeginthispositionasaSoftwareEngineer.\\nEducation\\n09/2019–05/2024 PostsandTelecommunicationsInstituteofTechnology .\\nInformationTechnology\\nGPA3.31/4.0\\nResearch Interests\\nNaturalLanguageProcessing(NLP),MachineLearningandDeepLearning.\\nWord Experience\\nMasterTraineratCentreforDevelopmentofAdvancedComputing(C-DAC),India(09/2024-\\npresent)\\n•ParticipateinArtiﬁcialIntelligence&DataScience:PG-DBDA-PostGraduateDiplomainBigData\\nAnalytics\\nAIResearcheratReseachInstituteofPostsandTelecommunications(RIPT)-PTIT(08/2024-\\npresent)\\n•Researchandwritingpaper\\n•BuildachatbotsystemusingRAGtechniques,FunctionCallingwithLLMs\\nSoftwareAIEngineeratA.I-SoftJSC(02/2023-08/2024)\\n•Buildwebsitesforresearchtopics.\\n•Buildachatbotsystemtosupportenrollment(End-to-End)usingRasaandBotpressFramework.\\n•Buildacrawldatamodule,dataanalysisusingBERTopicforsocialnetworklisteningsystem.\\n•ResearchpaperaboutAIandNaturalLanguageProcessingandapplytousedsoftwareproducts.\\n•Buildachatbotsystemtosupportenrollment(End-to-End)withLLMsusingRAG\\n(Retrieval-AugmentedGeneration)techniqueswithLangChainandLlamaIndexFramework.\\nSkills\\nLanguages English,Vietnamese(nativespeaker)\\nCoding Python,Linux,SQL,FastAPI,Docker,...\\nDatabases My sql,MongoDB,FAISS,Chroma...\\nWebDev Html, css ,JavaScript,Reactjs,Antdesign,MUI...', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5ce53d15-24ea-4770-9226-c024997c03a8', embedding=None, metadata={'page_label': '2', 'file_name': 'e:/Master-code/Job-recommend/CV-Analyzer-Job-Recommender/Multiple models/Dang_Quang_Dung_CV__Copy___Copy_.pdf', 'file_path': 'Dang_Quang_Dung_CV__Copy___Copy_.pdf', 'file_type': 'application/pdf', 'file_size': 763671, 'creation_date': '2025-01-19', 'last_modified_date': '2024-12-30', 'last_accessed_date': None}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Skills (continued)\\nOpen-sourceNLPTools Transformers(HuggingFace),spaCy,scikit-learn,Gensim,NLTK,BERTopic,\\nRasa,Botpress,Langchain,LlamaIndex...\\nDeepLearningFrameworks TensorFlow,Keras,PyTorch\\nMisc. Research Skills, Problem Solving, Report Writing, Teamwork and Indepen-\\ndently,DataStructuresandAlgorithms,DataVisualization(Tableau,Excel)\\nScientific research achievements\\nPublished scientific articles\\n2024 An intent-ﬁltered retrieval-augmented generation chatbot for university admissions in\\nVietnam .InternationalConferenceOnAdvancedTechnologiesForCommunications.\\nMiscellaneous Experience\\nCertification\\n11/2021 APPLICATIONSOFALGORITHM .byPostsandTelecommunicationsInstituteofTechnology\\nandSamsungElectronicsVietnamCompanyLimited.\\n09/2022 Toeic605 .byIIGVietNam.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "_resume_content = \"\\n\".join([doc.text for doc in documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dang Quang Dung\\nanhdungvk01@gmail.com\\n0869040236\\nLinkedin\\nGithub\\nCau Giay - Ha Noi\\nProfile\\nInﬁveyears’time,IwanttobecomeanAIEngineer.MystartingpointistobeginthispositionasaSoftwareEngineer.\\nEducation\\n09/2019–05/2024 PostsandTelecommunicationsInstituteofTechnology .\\nInformationTechnology\\nGPA3.31/4.0\\nResearch Interests\\nNaturalLanguageProcessing(NLP),MachineLearningandDeepLearning.\\nWord Experience\\nMasterTraineratCentreforDevelopmentofAdvancedComputing(C-DAC),India(09/2024-\\npresent)\\n•ParticipateinArtiﬁcialIntelligence&DataScience:PG-DBDA-PostGraduateDiplomainBigData\\nAnalytics\\nAIResearcheratReseachInstituteofPostsandTelecommunications(RIPT)-PTIT(08/2024-\\npresent)\\n•Researchandwritingpaper\\n•BuildachatbotsystemusingRAGtechniques,FunctionCallingwithLLMs\\nSoftwareAIEngineeratA.I-SoftJSC(02/2023-08/2024)\\n•Buildwebsitesforresearchtopics.\\n•Buildachatbotsystemtosupportenrollment(End-to-End)usingRasaandBotpressFramework.\\n•Buildacrawldatamodule,dataanalysisusingBERTopicforsocialnetworklisteningsystem.\\n•ResearchpaperaboutAIandNaturalLanguageProcessingandapplytousedsoftwareproducts.\\n•Buildachatbotsystemtosupportenrollment(End-to-End)withLLMsusingRAG\\n(Retrieval-AugmentedGeneration)techniqueswithLangChainandLlamaIndexFramework.\\nSkills\\nLanguages English,Vietnamese(nativespeaker)\\nCoding Python,Linux,SQL,FastAPI,Docker,...\\nDatabases My sql,MongoDB,FAISS,Chroma...\\nWebDev Html, css ,JavaScript,Reactjs,Antdesign,MUI...\\nSkills (continued)\\nOpen-sourceNLPTools Transformers(HuggingFace),spaCy,scikit-learn,Gensim,NLTK,BERTopic,\\nRasa,Botpress,Langchain,LlamaIndex...\\nDeepLearningFrameworks TensorFlow,Keras,PyTorch\\nMisc. Research Skills, Problem Solving, Report Writing, Teamwork and Indepen-\\ndently,DataStructuresandAlgorithms,DataVisualization(Tableau,Excel)\\nScientific research achievements\\nPublished scientific articles\\n2024 An intent-ﬁltered retrieval-augmented generation chatbot for university admissions in\\nVietnam .InternationalConferenceOnAdvancedTechnologiesForCommunications.\\nMiscellaneous Experience\\nCertification\\n11/2021 APPLICATIONSOFALGORITHM .byPostsandTelecommunicationsInstituteofTechnology\\nandSamsungElectronicsVietnamCompanyLimited.\\n09/2022 Toeic605 .byIIGVietNam.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_resume_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$defs': {'Education': {'properties': {'institution': {'anyOf': [{'type': 'string'},\n",
       "      {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'description': 'The name of the educational institution',\n",
       "     'title': 'Institution'},\n",
       "    'degree': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'description': 'The degree or qualification earned',\n",
       "     'title': 'Degree'},\n",
       "    'graduation_date': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'description': \"The graduation date (e.g., 'YYYY-MM')\",\n",
       "     'title': 'Graduation Date'},\n",
       "    'details': {'anyOf': [{'items': {'type': 'string'}, 'type': 'array'},\n",
       "      {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'description': 'Additional details about the education (e.g., coursework, achievements)',\n",
       "     'title': 'Details'}},\n",
       "   'title': 'Education',\n",
       "   'type': 'object'},\n",
       "  'Experience': {'properties': {'company': {'anyOf': [{'type': 'string'},\n",
       "      {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'description': 'The name of the company or organization',\n",
       "     'title': 'Company'},\n",
       "    'location': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'description': 'The location of the company or organization',\n",
       "     'title': 'Location'},\n",
       "    'role': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'description': 'The role or job title held by the candidate',\n",
       "     'title': 'Role'},\n",
       "    'start_date': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'description': \"The start date of the job (e.g., 'YYYY-MM')\",\n",
       "     'title': 'Start Date'},\n",
       "    'end_date': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'description': \"The end date of the job or 'Present' if ongoing (e.g., 'MM-YYYY')\",\n",
       "     'title': 'End Date'},\n",
       "    'responsibilities': {'anyOf': [{'items': {'type': 'string'},\n",
       "       'type': 'array'},\n",
       "      {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'description': 'A list of responsibilities and tasks handled during the job',\n",
       "     'title': 'Responsibilities'}},\n",
       "   'title': 'Experience',\n",
       "   'type': 'object'}},\n",
       " 'properties': {'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "   'default': None,\n",
       "   'description': 'The full name of the candidate',\n",
       "   'title': 'Name'},\n",
       "  'email': {'anyOf': [{'format': 'email', 'type': 'string'}, {'type': 'null'}],\n",
       "   'default': None,\n",
       "   'description': 'The email of the candidate',\n",
       "   'title': 'Email'},\n",
       "  'age': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "   'default': None,\n",
       "   'description': 'The age of the candidate.',\n",
       "   'title': 'Age'},\n",
       "  'skills': {'anyOf': [{'items': {'type': 'string'}, 'type': 'array'},\n",
       "    {'type': 'null'}],\n",
       "   'default': None,\n",
       "   'description': 'A list of high-level skills possessed by the candidate.',\n",
       "   'title': 'Skills'},\n",
       "  'experience': {'anyOf': [{'items': {'$ref': '#/$defs/Experience'},\n",
       "     'type': 'array'},\n",
       "    {'type': 'null'}],\n",
       "   'default': None,\n",
       "   'description': 'A list of experiences detailing previous jobs, roles, and responsibilities',\n",
       "   'title': 'Experience'},\n",
       "  'education': {'anyOf': [{'items': {'$ref': '#/$defs/Education'},\n",
       "     'type': 'array'},\n",
       "    {'type': 'null'}],\n",
       "   'default': None,\n",
       "   'description': 'A list of educational qualifications of the candidate including degrees, institutions studied in, and dates of start and end.',\n",
       "   'title': 'Education'}},\n",
       " 'title': 'Candidate',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing response: Expecting ',' delimiter: line 7 column 3 (char 1412)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to extract insights. Please ensure the resume and query engine are properly configured.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to get a response from LLM.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m parsed_data \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(Candidate\u001b[38;5;241m.\u001b[39mmodel_validate(parsed_data))\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\llamaindex\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\llamaindex\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\llamaindex\\lib\\json\\decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting ',' delimiter: line 7 column 3 (char 1412)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError parsing response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to extract insights. Please ensure the resume and query engine are properly configured.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     26\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to extract insights. Please ensure the resume and query engine are properly configured."
     ]
    }
   ],
   "source": [
    "# Store the pre-extracted content\n",
    "\n",
    "prompt = f\"\"\"\n",
    "    You are an expert in analyzing resumes. Use the following JSON schema to extract relevant information:\n",
    "    ```json\n",
    "    {output_schema}\n",
    "    ```json\n",
    "    Extract the information from the following document and provide a structured JSON response strictly adhering to the schema above. \n",
    "    Please remove any ```json ``` characters from the output. Do not make up any information. If a field cannot be extracted, mark it as `n/a`.\n",
    "    Document:\n",
    "    ----------------\n",
    "    {_resume_content}\n",
    "    ----------------\n",
    "    \"\"\"\n",
    "try:\n",
    "    response = llm_option.complete(prompt)\n",
    "    if not response or not response.text:\n",
    "        raise ValueError(\"Failed to get a response from LLM.\")\n",
    "\n",
    "    parsed_data = json.loads(response.text)\n",
    "    print(Candidate.model_validate(parsed_data))\n",
    "except Exception as e:\n",
    "    print(f\"Error parsing response: {str(e)}\")\n",
    "    raise ValueError(\n",
    "        \"Failed to extract insights. Please ensure the resume and query engine are properly configured.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"name\": \"Dang Quang Dung\",\\n  \"email\": \"anhdungvk01@gmail.com\",\\n  \"age\": \"n/a\",\\n  \"skills\": \"[\\\\\"English\\\\\", \\\\\"Vietnamese (native speaker)\\\\\", \\\\\"Python\\\\\", \\\\\"Linux\\\\\", \\\\\"SQL\\\\\", \\\\\"FastAPI\\\\\", \\\\\"Docker\\\\\", ..., \\\\\"BERTopic\\\\\", \\\\\"Rasa\\\\\", \\\\\"Botpress\\\\\", \\\\\"Langchain\\\\\", \\\\\"LlamaIndex\\\\\", \\\\\"TensorFlow\\\\\", \\\\\"Keras\\\\\", \\\\\"PyTorch\\\\\"]\",\\n  \"experience\": \"[{\\\\\"company\\\\\": \\\\\"Cau Giay - Ha Noi\\\\\", \\\\\"location\\\\\": \\\\\"Profile\\\\\", \\\\\"role\\\\\": \\\\\"Software Engineer\\\\\", \\\\\"start_date\\\\\": \\\\\"n/a\\\\\", \\\\\"end_date\\\\\": \\\\\"Present\\\\\", \\\\\"responsibilities\\\\\": \\\\\"n/a\\\\\"}, {\\\\\"company\\\\\": \\\\\"CentreforDevelopmentofAdvancedComputing (C-DAC), India\\\\\", \\\\\"location\\\\\": \\\\\"MasterTrainer role at C-DAC, India\\\\\", \\\\\"role\\\\\": \\\\\"AI Researcher and Master Trainer\\\\\", \\\\\"start_date\\\\\": \\\\\"09/2024\\\\\", \\\\\"end_date\\\\\": \\\\\"Present\\\\\", \\\\\"responsibilities\\\\\": \\\\\"Research and writing paper\\\\\"}, {\\\\\"company\\\\\": \\\\\"A.I-SoftJSC\\\\\", \\\\\"location\\\\\": \\\\\"Software AI Engineer at A.I-SoftJSC\\\\\", \\\\\"role\\\\\": \\\\\"Software AI Engineer\\\\\", \\\\\"start_date\\\\\": \\\\\"02/2023\\\\\", \\\\\"end_date\\\\\": \\\\\"08/2024\\\\\", \\\\\"responsibilities\\\\\": \\\\\"Build websites for research topics, Build chatbots using RAG techniques, Function Calling with LLMs\\\\\"}, {\\\\\"company\\\\\": \\\\\"Research Institute of Posts and Telecommunications (RIPT)-PTIT\\\\\", \\\\\"location\\\\\": \\\\\"AI Researcher at RIPT-PTIT\\\\\", \\\\\"role\\\\\": \\\\\"AI Researcher\\\\\", \\\\\"start_date\\\\\": \\\\\"08/2024\\\\\", \\\\\"end_date\\\\\": \\\\\"Present\\\\\", \\\\\"responsibilities\\\\\": \\\\\"Research paper, Build chatbots system using RAG techniques\\\\\"}],\"\\n  \"education\": \"[{\\\\\"institution\\\\\": \\\\\"Posts and Telecommunications Institute of Technology\\\\\", \\\\\"degree\\\\\": \\\\\"Information Technology\\\\\", \\\\\"graduation_date\\\\\": \\\\\"05/2024\\\\\", \\\\\"details\\\\\": \\\\\"GPA3.31/4.0, Research Interests: Natural Language Processing (NLP), Machine Learning and Deep Learning\\\\\"}]\",\\n\\n  \"age\": \"n/a\",\\n  \"name\": \"Dang Quang Dung\",\\n  \"email\": \"anhdungvk01@gmail.com\",\\n  \"skills\": \"[\\\\\"English\\\\\", \\\\\"Vietnamese (native speaker)\\\\\", \\\\\"Python\\\\\", \\\\\"Linux\\\\\", \\\\\"SQL\\\\\", \\\\\"FastAPI\\\\\", \\\\\"Docker\\\\\", ..., \\\\\"BERTopic\\\\\", \\\\\"Rasa\\\\\", \\\\\"Botpress\\\\\", \\\\\"Langchain\\\\\", \\\\\"LlamaIndex\\\\\", \\\\\"TensorFlow\\\\\", \\\\\"Keras\\\\\", \\\\\"PyTorch\\\\\"]\",\\n  \"experience\": \"[{\\\\\"company\\\\\": \\\\\"Cau Giay - Ha Noi\\\\\", \\\\\"location\\\\\": \\\\\"Profile\\\\\", \\\\\"role\\\\\": \\\\\"Software Engineer\\\\\", \\\\\"start_date\\\\\": \\\\\"n/a\\\\\", \\\\\"end_date\\\\\": \\\\\"Present\\\\\", \\\\\"responsibilities\\\\\": \\\\\"n/a\\\\\"}, {\\\\\"company\\\\\": \\\\\"CentreforDevelopmentofAdvancedComputing (C-DAC), India\\\\\", \\\\\"location\\\\\": \\\\\"MasterTrainer role at C-DAC, India\\\\\", \\\\\"role\\\\\": \\\\\"AI Researcher and Master Trainer\\\\\", \\\\\"start_date\\\\\": \\\\\"09/2024\\\\\", \\\\\"end_date\\\\\": \\\\\"Present\\\\\", \\\\\"responsibilities\\\\\": \\\\\"Research and writing paper\\\\\"}, {\\\\\"company\\\\\": \\\\\"A.I-SoftJSC\\\\\", \\\\\"location\\\\\": \\\\\"Software AI Engineer at A.I-SoftJSC\\\\\", \\\\\"role\\\\\": \\\\\"Software AI Engineer\\\\\", \\\\\"start_date\\\\\": \\\\\"02/2023\\\\\", \\\\\"end_date\\\\\": \\\\\"08/2024\\\\\", \\\\\"responsibilities\\\\\": \\\\\"Build websites for research topics, Build chatbots using RAG techniques, Function Calling with LLMs\\\\\"}, {\\\\\"company\\\\\": \\\\\"Research Institute of Posts and Telecommunications (RIPT)-PTIT\\\\\", \\\\\"location\\\\\": \\\\\"AI Researcher at RIPT-PTIT\\\\\", \\\\\"role\\\\\": \\\\\"AI Researcher\\\\\", \\\\\"start_date\\\\\": \\\\\"08/2024\\\\\", \\\\\"end_date\\\\\": \\\\\"Present\\\\\", \\\\\"responsibilities\\\\\": \\\\\"Research paper, Build chatbots system using RAG techniques\\\\\"}],\"\\n  \"education\": \"[{\\\\\"institution\\\\\": \\\\\"Posts and Telecommunications Institute of Technology\\\\\", \\\\\"degree\\\\\": \\\\\"Information Technology\\\\\", \\\\\"graduation_date\\\\\": \\\\\"05/2024\\\\\", \\\\\"details\\\\\": \\\\\"GPA3.31/4.0, Research Interests: Natural Language Processing (NLP), Machine Learning and Deep Learning\\\\\"}]\"'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting ',' delimiter: line 7 column 3 (char 1412)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m parsed_data \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m parsed_data\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\llamaindex\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\llamaindex\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\llamaindex\\lib\\json\\decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    350\u001b[0m \n\u001b[0;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting ',' delimiter: line 7 column 3 (char 1412)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "parsed_data = json.loads(response.text)\n",
    "parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parsed_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(Candidate\u001b[38;5;241m.\u001b[39mmodel_validate(\u001b[43mparsed_data\u001b[49m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'parsed_data' is not defined"
     ]
    }
   ],
   "source": [
    "print(Candidate.model_validate(parsed_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
